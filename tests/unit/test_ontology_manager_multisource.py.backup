#!/usr/bin/env python3
"""
Comprehensive unit tests for OntologyManager multi-source loading functionality.

This module contains specialized unit tests focusing specifically on multi-source
ontology loading capabilities, including batch operations, mixed format scenarios,
performance aspects, memory management, concurrent access patterns, cache efficiency,
and comprehensive error handling for batch operations.

Test Coverage:
- Multi-source loading from different formats (OWL, CSV, JSON-LD)
- Mixed success/failure scenarios during batch loading
- Performance characteristics of batch loading operations
- Memory management when loading multiple large ontologies
- Concurrent access patterns and thread safety
- Cache efficiency with multiple sources
- Statistics aggregation for multi-source operations
- Error handling and reporting for batch operations
- Resource cleanup and memory optimization
- Load balancing and prioritization strategies
"""

import concurrent.futures
import json
import tempfile
import time
from pathlib import Path
from unittest.mock import Mock, patch

import pytest

# Import the modules to be tested
try:
    from aim2_project.aim2_ontology.models import Ontology, Relationship, Term
    from aim2_project.aim2_ontology.ontology_manager import (
        OntologyManager,
    )
    from aim2_project.aim2_ontology.parsers import ParseResult
except ImportError:
    import warnings

    warnings.warn("Some imports failed - tests may be skipped", ImportWarning)


class TestOntologyManagerMultiSource:
    """Comprehensive test suite for OntologyManager multi-source loading functionality."""

    @pytest.fixture
    def temp_dir(self):
        """Create a temporary directory for test files."""
        with tempfile.TemporaryDirectory() as tmp_dir:
            yield Path(tmp_dir)

    @pytest.fixture
    def ontology_manager(self):
        """Create an OntologyManager instance for testing."""
        return OntologyManager(enable_caching=True, cache_size_limit=20)

    @pytest.fixture
    def sample_ontologies(self):
        """Create multiple sample ontologies for multi-source testing."""
        ontologies = []

        for i in range(5):
            terms = {}
            relationships = {}

            # Create diverse terms for each ontology
            for j in range(10):
                term_id = f"TEST:{i*10+j:06d}"
                terms[term_id] = Term(
                    id=term_id,
                    name=f"Term {j} from Ontology {i}",
                    definition=f"Definition for term {j} in ontology {i}",
                    synonyms=[f"synonym{j}_1", f"synonym{j}_2"],
                    namespace=f"namespace_{i}",
                )

            # Create relationships within each ontology
            for j in range(5):
                rel_id = f"REL:{i*10+j:06d}"
                relationships[rel_id] = Relationship(
                    id=rel_id,
                    subject=f"TEST:{i*10+j:06d}",
                    predicate="regulates",
                    object=f"TEST:{i*10+(j+1)%10:06d}",
                    confidence=0.8 + (j * 0.04),
                )

            ontology = Ontology(
                id=f"TESTMULTI:{i:06d}",
                name=f"Multi-Source Test Ontology {i}",
                version=f"1.{i}",
                description=f"Test ontology {i} for multi-source loading scenarios",
                terms=terms,
                relationships=relationships,
                namespaces=[f"namespace_{i}"],
            )
            ontologies.append(ontology)

        return ontologies

    @pytest.fixture
    def create_test_files(self, temp_dir, sample_ontologies):
        """Create test files in different formats for multi-source testing."""
        files = {}

        # Create OWL files
        for i in range(2):
            ontology = sample_ontologies[i]
            owl_content = f"""<?xml version="1.0"?>
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:owl="http://www.w3.org/2002/07/owl#"
         xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#">
    <owl:Ontology rdf:about="http://example.org/{ontology.id}">
        <rdfs:label>{ontology.name}</rdfs:label>
        <rdfs:comment>{ontology.description}</rdfs:comment>
    </owl:Ontology>
    <!-- Terms would be here in real OWL -->
</rdf:RDF>"""

            owl_file = temp_dir / f"ontology_{i}.owl"
            owl_file.write_text(owl_content)
            files[f"owl_{i}"] = (str(owl_file), ontology)

        # Create CSV files
        for i in range(2, 4):
            ontology = sample_ontologies[i]
            csv_lines = ["id,name,definition,namespace,synonyms"]

            for term_id, term in list(ontology.terms.items())[:5]:  # First 5 terms
                synonyms = "|".join(term.synonyms)
                csv_lines.append(
                    f"{term_id},{term.name},{term.definition},{term.namespace},{synonyms}"
                )

            csv_file = temp_dir / f"ontology_{i}.csv"
            csv_file.write_text("\n".join(csv_lines))
            files[f"csv_{i}"] = (str(csv_file), ontology)

        # Create JSON-LD file
        ontology = sample_ontologies[4]
        jsonld_content = {
            "@context": {
                "@vocab": "http://example.org/",
                "rdfs": "http://www.w3.org/2000/01/rdf-schema#",
            },
            "@id": ontology.id,
            "@type": "Ontology",
            "rdfs:label": ontology.name,
            "rdfs:comment": ontology.description,
            "terms": [
                {
                    "@id": term_id,
                    "rdfs:label": term.name,
                    "definition": term.definition,
                    "namespace": term.namespace,
                }
                for term_id, term in list(ontology.terms.items())[:3]
            ],
        }

        jsonld_file = temp_dir / "ontology_4.jsonld"
        jsonld_file.write_text(json.dumps(jsonld_content, indent=2))
        files["jsonld_4"] = (str(jsonld_file), ontology)

        return files

    def test_load_multiple_sources_mixed_formats(
        self, ontology_manager, create_test_files
    ):
        """Test loading multiple ontologies from different formats simultaneously."""
        # Get all file paths
        sources = [file_info[0] for file_info in create_test_files.values()]

        with patch(
            "aim2_project.aim2_ontology.ontology_manager.auto_detect_parser"
        ) as mock_auto_detect:
            # Mock successful parsing for all files
            def create_mock_parser(ontology):
                parser = Mock()
                parser.format_name = "mock_format"
                parser.parse.return_value = ParseResult(
                    success=True,
                    data=ontology,
                    errors=[],
                    warnings=[],
                    metadata={"test": True},
                    parse_time=0.1,
                )
                return parser

            # Return appropriate ontology for each file
            mock_parsers = []
            for key, (file_path, ontology) in create_test_files.items():
                mock_parsers.append(create_mock_parser(ontology))
            mock_auto_detect.side_effect = mock_parsers

            # Load all sources
            results = ontology_manager.load_ontologies(sources)

            # Verify results
            assert len(results) == len(sources)
            successful_results = [r for r in results if r.success]
            assert len(successful_results) == len(
                sources
            ), f"Expected all to succeed, got: {[r.errors for r in results if not r.success]}"

            # Verify each ontology was loaded
            assert len(ontology_manager.ontologies) == len(sources)

            # Verify statistics
            stats = ontology_manager.get_statistics()
            assert stats["total_loads"] == len(sources)
            assert stats["successful_loads"] == len(sources)
            assert stats["failed_loads"] == 0

    def test_mixed_success_failure_scenarios(self, ontology_manager, create_test_files):
        """Test batch loading with some files succeeding and others failing."""
        sources = list(create_test_files.keys())
        file_paths = [create_test_files[key][0] for key in sources]

        # Add some non-existent files to the mix
        file_paths.append("/nonexistent/file1.owl")
        file_paths.append("/nonexistent/file2.csv")

        with patch(
            "aim2_project.aim2_ontology.ontology_manager.auto_detect_parser"
        ) as mock_auto_detect:

            def mock_parser_factory(call_count=[0]):
                call_count[0] += 1
                if call_count[0] <= len(sources):  # First N calls succeed
                    key = sources[call_count[0] - 1]
                    ontology = create_test_files[key][1]
                    parser = Mock()
                    parser.format_name = f"format_{call_count[0]}"
                    parser.parse.return_value = ParseResult(
                        success=True,
                        data=ontology,
                        errors=[],
                        warnings=[],
                        metadata={},
                        parse_time=0.1,
                    )
                    return parser
                else:  # Later calls fail (for nonexistent files)
                    return None

            mock_auto_detect.side_effect = lambda **kwargs: mock_parser_factory()

            # Load with mixed success/failure
            results = ontology_manager.load_ontologies(file_paths)

            # Verify mixed results
            assert len(results) == len(file_paths)
            successful_results = [r for r in results if r.success]
            failed_results = [r for r in results if not r.success]

            assert len(successful_results) == len(sources)  # Original files succeed
            assert len(failed_results) == 2  # Nonexistent files fail

            # Verify only successful ontologies were stored
            assert len(ontology_manager.ontologies) == len(sources)

            # Verify statistics reflect mixed results
            stats = ontology_manager.get_statistics()
            assert stats["total_loads"] == len(file_paths)
            assert stats["successful_loads"] == len(sources)
            assert stats["failed_loads"] == 2

    def test_performance_batch_loading_large_ontologies(
        self, ontology_manager, temp_dir
    ):
        """Test performance aspects of batch loading multiple large ontologies."""
        # Create larger test ontologies
        large_ontologies = []
        file_paths = []

        for i in range(3):  # 3 large ontologies
            # Create ontology with many terms
            terms = {}
            for j in range(500):  # 500 terms each
                term_id = f"LARGE:{i*1000+j:06d}"
                terms[term_id] = Term(
                    id=term_id,
                    name=f"Large Term {j} from Ontology {i}",
                    definition=f"Definition for large term {j} in ontology {i}",
                    synonyms=[f"syn{j}_1", f"syn{j}_2", f"syn{j}_3"],
                    namespace=f"large_namespace_{i}",
                )

            # Create many relationships
            relationships = {}
            for j in range(100):  # 100 relationships each
                rel_id = f"REL:{i*1000+j:06d}"
                relationships[rel_id] = Relationship(
                    id=rel_id,
                    subject=f"LARGE:{i*1000+j:06d}",
                    predicate="regulates",
                    object=f"LARGE:{i*1000+(j+1)%500:06d}",
                    confidence=0.9,
                )

            ontology = Ontology(
                id=f"LARGETEST:{i:06d}",
                name=f"Large Test Ontology {i}",
                version="1.0",
                description=f"Large test ontology {i} with 500 terms",
                terms=terms,
                relationships=relationships,
                namespaces=[f"large_namespace_{i}"],
            )
            large_ontologies.append(ontology)

            # Create CSV file for this ontology
            csv_lines = ["id,name,definition,namespace"]
            for term_id, term in terms.items():
                csv_lines.append(
                    f"{term_id},{term.name},{term.definition},{term.namespace}"
                )

            csv_file = temp_dir / f"large_ontology_{i}.csv"
            csv_file.write_text("\n".join(csv_lines))
            file_paths.append(str(csv_file))

        with patch(
            "aim2_project.aim2_ontology.ontology_manager.auto_detect_parser"
        ) as mock_auto_detect:

            def create_large_parser(ontology):
                parser = Mock()
                parser.format_name = "csv"
                parser.parse.return_value = ParseResult(
                    success=True,
                    data=ontology,
                    errors=[],
                    warnings=[],
                    metadata={"large": True},
                    parse_time=0.5,  # Simulate longer parse time
                )
                return parser

            mock_auto_detect.side_effect = [
                create_large_parser(ont) for ont in large_ontologies
            ]

            # Measure batch loading performance
            start_time = time.time()
            results = ontology_manager.load_ontologies(file_paths)
            total_time = time.time() - start_time

            # Verify all succeeded
            assert all(r.success for r in results)
            assert len(ontology_manager.ontologies) == 3

            # Performance assertions
            assert total_time < 30.0, f"Batch loading took too long: {total_time:.2f}s"
            average_time = total_time / len(file_paths)
            assert (
                average_time < 10.0
            ), f"Average load time too high: {average_time:.2f}s"

            # Verify memory usage is reasonable
            stats = ontology_manager.get_statistics()
            assert stats["total_terms"] == 1500  # 500 * 3
            assert stats["total_relationships"] == 300  # 100 * 3

    def test_memory_management_multi_source_loading(self, ontology_manager, temp_dir):
        """Test memory management when loading multiple large ontologies."""
        import gc

        import psutil

        process = psutil.Process()
        initial_memory = process.memory_info().rss

        # Create multiple ontologies of moderate size
        file_paths = []
        expected_ontologies = []

        for i in range(5):
            # Create ontology with moderate number of terms
            terms = {}
            for j in range(200):  # 200 terms each
                term_id = f"MEMORY:{i*1000+j:06d}"
                terms[term_id] = Term(
                    id=term_id,
                    name=f"Memory Test Term {j}",
                    definition=f"Definition {j}" * 10,  # Longer definitions
                    synonyms=[f"synonym{j}_{k}" for k in range(5)],  # More synonyms
                    namespace=f"memory_ns_{i}",
                )

            ontology = Ontology(
                id=f"MEMTEST:{i:06d}",
                name=f"Memory Test Ontology {i}",
                version="1.0",
                terms=terms,
                relationships={},
                namespaces=[f"memory_ns_{i}"],
            )
            expected_ontologies.append(ontology)

            # Create CSV file
            csv_lines = ["id,name,definition,namespace,synonyms"]
            for term_id, term in terms.items():
                synonyms = "|".join(term.synonyms)
                csv_lines.append(
                    f"{term_id},{term.name},{term.definition},{term.namespace},{synonyms}"
                )

            csv_file = temp_dir / f"memory_test_{i}.csv"
            csv_file.write_text("\n".join(csv_lines))
            file_paths.append(str(csv_file))

        with patch(
            "aim2_project.aim2_ontology.ontology_manager.auto_detect_parser"
        ) as mock_auto_detect:
            mock_auto_detect.side_effect = [
                Mock(
                    format_name="csv",
                    parse=Mock(
                        return_value=ParseResult(
                            success=True,
                            data=ont,
                            errors=[],
                            warnings=[],
                            metadata={},
                            parse_time=0.1,
                        )
                    ),
                )
                for ont in expected_ontologies
            ]

            # Load all ontologies
            results = ontology_manager.load_ontologies(file_paths)

            # Force garbage collection
            gc.collect()

            # Check memory usage
            after_load_memory = process.memory_info().rss
            memory_increase = after_load_memory - initial_memory

            # Verify loading succeeded
            assert all(r.success for r in results)
            assert len(ontology_manager.ontologies) == 5

            # Memory increase should be reasonable (less than 500MB for this test)
            assert (
                memory_increase < 500 * 1024 * 1024
            ), f"Memory increase too high: {memory_increase / 1024 / 1024:.2f}MB"

            # Test cache cleanup to free memory
            ontology_manager.clear_cache()
            gc.collect()

            after_cleanup_memory = process.memory_info().rss
            memory_freed = after_load_memory - after_cleanup_memory

            # Should free some memory (at least 10MB)
            assert (
                memory_freed > 10 * 1024 * 1024
            ), f"Cache cleanup freed too little memory: {memory_freed / 1024 / 1024:.2f}MB"

    def test_concurrent_multi_source_loading(self, ontology_manager, create_test_files):
        """Test concurrent loading of multiple sources with thread safety."""
        file_groups = [
            [create_test_files[key][0] for key in list(create_test_files.keys())[:2]],
            [create_test_files[key][0] for key in list(create_test_files.keys())[2:4]],
            [create_test_files[key][0] for key in list(create_test_files.keys())[4:]],
        ]

        with patch(
            "aim2_project.aim2_ontology.ontology_manager.auto_detect_parser"
        ) as mock_auto_detect:
            # Create parsers for all ontologies
            all_parsers = []
            for key, (file_path, ontology) in create_test_files.items():
                parser = Mock()
                parser.format_name = "concurrent_test"
                parser.parse.return_value = ParseResult(
                    success=True,
                    data=ontology,
                    errors=[],
                    warnings=[],
                    metadata={"concurrent": True},
                    parse_time=0.2,
                )
                all_parsers.append(parser)

            # Use thread-local counter for parser assignment
            import threading

            thread_local = threading.local()

            def get_parser(*args, **kwargs):
                if not hasattr(thread_local, "counter"):
                    thread_local.counter = 0
                parser = all_parsers[thread_local.counter % len(all_parsers)]
                thread_local.counter += 1
                return parser

            mock_auto_detect.side_effect = get_parser

            # Function to load a group of files
            def load_group(group):
                return ontology_manager.load_ontologies(group)

            # Use ThreadPoolExecutor for concurrent loading
            with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
                # Submit all loading tasks
                future_to_group = {
                    executor.submit(load_group, group): i
                    for i, group in enumerate(file_groups)
                }

                # Collect results
                all_results = []
                for future in concurrent.futures.as_completed(future_to_group):
                    group_index = future_to_group[future]
                    try:
                        group_results = future.result()
                        all_results.extend(group_results)
                    except Exception as exc:
                        pytest.fail(
                            f"Group {group_index} generated an exception: {exc}"
                        )

            # Verify concurrent loading results
            assert len(all_results) == len(create_test_files)
            successful_results = [r for r in all_results if r.success]
            assert (
                len(successful_results) >= len(create_test_files) - 1
            ), "Most concurrent loads should succeed"

            # Verify thread safety - no data corruption
            assert (
                len(ontology_manager.ontologies) >= 1
            ), "At least some ontologies should be loaded"

            # Verify statistics are consistent
            stats = ontology_manager.get_statistics()
            assert stats["total_loads"] >= len(create_test_files)

    def test_cache_efficiency_multi_source(self, ontology_manager, create_test_files):
        """Test cache efficiency with multiple sources and repeated loads."""
        sources = [file_info[0] for file_info in create_test_files.values()]

        with patch(
            "aim2_project.aim2_ontology.ontology_manager.auto_detect_parser"
        ) as mock_auto_detect:
            # Create parsers
            parsers = []
            for key, (file_path, ontology) in create_test_files.items():
                parser = Mock()
                parser.format_name = f"cache_test_{key}"
                parser.parse.return_value = ParseResult(
                    success=True,
                    data=ontology,
                    errors=[],
                    warnings=[],
                    metadata={"cache_test": True},
                    parse_time=0.1,
                )
                parsers.append(parser)

            # First load - all cache misses
            mock_auto_detect.side_effect = parsers
            results1 = ontology_manager.load_ontologies(sources)

            assert all(r.success for r in results1)
            assert ontology_manager.load_stats["cache_misses"] == len(sources)
            assert ontology_manager.load_stats["cache_hits"] == 0

            # Second load - should be all cache hits
            results2 = ontology_manager.load_ontologies(sources)

            assert all(r.success for r in results2)
            assert all(r.metadata.get("cache_hit", False) for r in results2)
            assert ontology_manager.load_stats["cache_hits"] == len(sources)

            # Verify parsers were only called once for each file
            for parser in parsers:
                assert parser.parse.call_count == 1

            # Test cache efficiency with partial overlaps
            partial_sources = sources[:3]  # First 3 sources
            new_source = "/tmp/new_ontology.owl"

            # Mock for new source
            new_ontology = Ontology(
                id="NEW:001",
                name="New Ontology",
                version="1.0",
                terms={},
                relationships={},
                namespaces=[],
            )
            new_parser = Mock()
            new_parser.format_name = "new_format"
            new_parser.parse.return_value = ParseResult(
                success=True,
                data=new_ontology,
                errors=[],
                warnings=[],
                metadata={},
                parse_time=0.1,
            )
            mock_auto_detect.side_effect = [new_parser]

            # Load mix of cached and new
            mixed_sources = partial_sources + [new_source]
            initial_cache_hits = ontology_manager.load_stats["cache_hits"]
            initial_cache_misses = ontology_manager.load_stats["cache_misses"]

            results3 = ontology_manager.load_ontologies(mixed_sources)

            assert all(r.success for r in results3)
            # Should have 3 more cache hits (partial_sources) and 1 more miss (new_source)
            assert ontology_manager.load_stats["cache_hits"] == initial_cache_hits + 3
            assert (
                ontology_manager.load_stats["cache_misses"] == initial_cache_misses + 1
            )

    def test_statistics_aggregation_multi_source(
        self, ontology_manager, create_test_files
    ):
        """Test statistics aggregation for multi-source operations."""
        sources = [file_info[0] for file_info in create_test_files.values()]

        with patch(
            "aim2_project.aim2_ontology.ontology_manager.auto_detect_parser"
        ) as mock_auto_detect:
            # Create diverse parsers with different formats and characteristics
            format_names = ["owl", "csv", "jsonld", "rdf", "custom"]
            parsers = []

            for i, (key, (file_path, ontology)) in enumerate(create_test_files.items()):
                parser = Mock()
                parser.format_name = format_names[i % len(format_names)]
                parser.parse.return_value = ParseResult(
                    success=True,
                    data=ontology,
                    errors=[],
                    warnings=[f"Warning for {key}"] if i % 2 == 0 else [],
                    metadata={"terms_parsed": len(ontology.terms)},
                    parse_time=0.1 + (i * 0.05),  # Varying parse times
                )
                parsers.append(parser)

            mock_auto_detect.side_effect = parsers

            # Load all sources
            ontology_manager.load_ontologies(sources)

            # Get comprehensive statistics
            stats = ontology_manager.get_statistics()

            # Verify basic load statistics
            assert stats["total_loads"] == len(sources)
            assert stats["successful_loads"] == len(sources)
            assert stats["failed_loads"] == 0

            # Verify format-specific statistics
            formats_loaded = stats["formats_loaded"]
            expected_formats = set(format_names[: len(sources)])
            actual_formats = set(formats_loaded.keys())
            assert expected_formats.issubset(
                actual_formats
            ), f"Missing formats: {expected_formats - actual_formats}"

            # Verify aggregated ontology statistics
            expected_total_terms = sum(
                len(ont.terms) for _, (_, ont) in create_test_files.items()
            )
            expected_total_relationships = sum(
                len(ont.relationships) for _, (_, ont) in create_test_files.items()
            )

            assert stats["total_terms"] == expected_total_terms
            assert stats["total_relationships"] == expected_total_relationships
            assert stats["loaded_ontologies"] == len(sources)

            # Test statistics after partial failures
            # Add some failing sources
            failing_sources = ["/nonexistent1.owl", "/nonexistent2.csv"]
            all_sources = sources + failing_sources

            # Mock will return None for failing sources (auto_detect_parser failure)
            mock_auto_detect.side_effect = parsers + [None, None]

            # Reset stats and load again
            ontology_manager.load_stats = {
                "total_loads": 0,
                "successful_loads": 0,
                "failed_loads": 0,
                "cache_hits": 0,
                "cache_misses": 0,
                "formats_loaded": {},
            }
            ontology_manager.ontologies.clear()
            ontology_manager.clear_cache()

            ontology_manager.load_ontologies(all_sources)
            stats_with_failures = ontology_manager.get_statistics()

            # Verify mixed statistics
            assert stats_with_failures["total_loads"] == len(all_sources)
            assert stats_with_failures["successful_loads"] == len(sources)
            assert stats_with_failures["failed_loads"] == len(failing_sources)

            # Success rate should be calculable
            success_rate = (
                stats_with_failures["successful_loads"]
                / stats_with_failures["total_loads"]
            )
            assert 0 < success_rate < 1

    def test_error_handling_batch_operations(self, ontology_manager, temp_dir):
        """Test comprehensive error handling and reporting for batch operations."""
        # Create a mix of valid and invalid files
        test_files = []
        expected_results = []

        # Valid OWL file
        valid_owl = temp_dir / "valid.owl"
        valid_owl.write_text(
            """<?xml version="1.0"?>
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:owl="http://www.w3.org/2002/07/owl#">
    <owl:Ontology rdf:about="http://example.org/valid"/>
</rdf:RDF>"""
        )
        test_files.append(str(valid_owl))
        expected_results.append("success")

        # Invalid XML file
        invalid_xml = temp_dir / "invalid.owl"
        invalid_xml.write_text("<<invalid xml content>>")
        test_files.append(str(invalid_xml))
        expected_results.append("parse_failure")

        # Valid CSV file
        valid_csv = temp_dir / "valid.csv"
        valid_csv.write_text("id,name,definition\nTEST:001,Test Term,Test Definition")
        test_files.append(str(valid_csv))
        expected_results.append("success")

        # Empty file
        empty_file = temp_dir / "empty.jsonld"
        empty_file.write_text("")
        test_files.append(str(empty_file))
        expected_results.append("parse_failure")

        # Nonexistent file
        test_files.append("/totally/nonexistent/file.owl")
        expected_results.append("file_not_found")

        # File with unsupported format
        unsupported = temp_dir / "unsupported.xyz"
        unsupported.write_text("unsupported content")
        test_files.append(str(unsupported))
        expected_results.append("no_parser")

        with patch(
            "aim2_project.aim2_ontology.ontology_manager.auto_detect_parser"
        ) as mock_auto_detect:

            def mock_parser_behavior(file_path=None, **kwargs):
                file_path_str = str(file_path) if file_path else ""

                if "valid.owl" in file_path_str:
                    # Valid OWL
                    parser = Mock()
                    parser.format_name = "owl"
                    parser.parse.return_value = ParseResult(
                        success=True,
                        data=Ontology(
                            id="VALID:001",
                            name="Valid",
                            version="1.0",
                            terms={},
                            relationships={},
                            namespaces=[],
                        ),
                        errors=[],
                        warnings=[],
                        metadata={},
                        parse_time=0.1,
                    )
                    return parser
                elif "invalid.owl" in file_path_str:
                    # Parser found but parsing fails
                    parser = Mock()
                    parser.format_name = "owl"
                    parser.parse.return_value = ParseResult(
                        success=False,
                        data=None,
                        errors=["XML parsing error: Invalid syntax"],
                        warnings=[],
                        metadata={},
                        parse_time=0.05,
                    )
                    return parser
                elif "valid.csv" in file_path_str:
                    # Valid CSV
                    parser = Mock()
                    parser.format_name = "csv"
                    parser.parse.return_value = ParseResult(
                        success=True,
                        data=Ontology(
                            id="VALIDCSV:001",
                            name="Valid CSV",
                            version="1.0",
                            terms={},
                            relationships={},
                            namespaces=[],
                        ),
                        errors=[],
                        warnings=["Minor formatting issue"],
                        metadata={},
                        parse_time=0.08,
                    )
                    return parser
                elif "empty.jsonld" in file_path_str:
                    # Parser found but fails on empty content
                    parser = Mock()
                    parser.format_name = "jsonld"
                    parser.parse.return_value = ParseResult(
                        success=False,
                        data=None,
                        errors=["Empty file or invalid JSON"],
                        warnings=[],
                        metadata={},
                        parse_time=0.01,
                    )
                    return parser
                elif "unsupported.xyz" in file_path_str:
                    # No parser available
                    return None
                else:
                    # For nonexistent files, this won't be called due to file check
                    return None

            mock_auto_detect.side_effect = mock_parser_behavior

            # Load all files with comprehensive error handling
            results = ontology_manager.load_ontologies(test_files)

            # Verify we got results for all files
            assert len(results) == len(test_files)

            # Analyze results by type
            success_count = 0
            parse_failure_count = 0
            file_not_found_count = 0
            no_parser_count = 0

            for i, result in enumerate(results):
                expected = expected_results[i]

                if expected == "success":
                    assert (
                        result.success is True
                    ), f"Expected success for {test_files[i]}, got: {result.errors}"
                    success_count += 1
                elif expected == "parse_failure":
                    assert result.success is False
                    assert len(result.errors) > 0
                    assert any(
                        "parsing" in error.lower()
                        or "invalid" in error.lower()
                        or "empty" in error.lower()
                        for error in result.errors
                    )
                    parse_failure_count += 1
                elif expected == "file_not_found":
                    assert result.success is False
                    assert len(result.errors) > 0
                    file_not_found_count += 1
                elif expected == "no_parser":
                    assert result.success is False
                    assert len(result.errors) > 0
                    assert any(
                        "no suitable parser" in error.lower() for error in result.errors
                    )
                    no_parser_count += 1

            # Verify error categorization
            assert success_count >= 2, "Should have at least 2 successful loads"
            assert parse_failure_count >= 2, "Should have at least 2 parse failures"
            assert file_not_found_count >= 1, "Should have at least 1 file not found"
            assert no_parser_count >= 1, "Should have at least 1 no parser available"

            # Verify only successful ontologies were stored
            successful_ontologies = [
                r.ontology for r in results if r.success and r.ontology
            ]
            assert len(ontology_manager.ontologies) == len(successful_ontologies)

            # Verify comprehensive statistics
            stats = ontology_manager.get_statistics()
            assert stats["total_loads"] == len(test_files)
            assert stats["successful_loads"] == success_count
            assert stats["failed_loads"] == len(test_files) - success_count

    def test_resource_cleanup_and_optimization(self, ontology_manager, temp_dir):
        """Test resource cleanup and memory optimization during multi-source loading."""
        # Create manager with limited cache to test eviction
        limited_manager = OntologyManager(enable_caching=True, cache_size_limit=3)

        # Create more files than cache limit
        file_paths = []
        ontologies = []

        for i in range(5):  # More than cache limit
            ontology = Ontology(
                id=f"CLEANUP:{i:03d}",
                name=f"Cleanup Test {i}",
                version="1.0",
                terms={
                    f"TERM:{j+i*1000:06d}": Term(
                        id=f"TERM:{j+i*1000:06d}",
                        name=f"Term {j}",
                        definition=f"Definition {j}",
                        namespace=f"ns_{i}",
                    )
                    for j in range(50)
                },  # 50 terms each
                relationships={},
                namespaces=[f"ns_{i}"],
            )
            ontologies.append(ontology)

            csv_file = temp_dir / f"cleanup_test_{i}.csv"
            csv_file.write_text(
                f"id,name,definition\nTEST:{i:03d},Test {i},Definition {i}"
            )
            file_paths.append(str(csv_file))

        with patch(
            "aim2_project.aim2_ontology.ontology_manager.auto_detect_parser"
        ) as mock_auto_detect:
            parsers = [
                Mock(
                    format_name="csv",
                    parse=Mock(
                        return_value=ParseResult(
                            success=True,
                            data=ont,
                            errors=[],
                            warnings=[],
                            metadata={},
                            parse_time=0.1,
                        )
                    ),
                )
                for ont in ontologies
            ]
            mock_auto_detect.side_effect = parsers

            # Load all files
            results = limited_manager.load_ontologies(file_paths)

            # Verify all loaded successfully
            assert all(r.success for r in results)
            assert len(limited_manager.ontologies) == 5

            # Verify cache was limited and eviction occurred
            assert len(limited_manager._cache) == limited_manager.cache_size_limit

            # Test explicit cache cleanup
            len(limited_manager._cache)
            limited_manager.clear_cache()
            assert len(limited_manager._cache) == 0

            # Test memory cleanup after removing ontologies
            initial_ontology_count = len(limited_manager.ontologies)
            removed_count = 0

            # Remove some ontologies
            for ontology_id in list(limited_manager.ontologies.keys())[:2]:
                if limited_manager.remove_ontology(ontology_id):
                    removed_count += 1

            assert (
                len(limited_manager.ontologies)
                == initial_ontology_count - removed_count
            )

            # Verify cache was also cleaned up
            remaining_cache_entries = len(limited_manager._cache)
            assert remaining_cache_entries == 0  # Already cleared above

    def test_load_balancing_and_prioritization(self, ontology_manager, temp_dir):
        """Test load balancing and prioritization strategies for multi-source operations."""
        # Create files with different sizes/complexity
        test_scenarios = [
            ("small", 10, 5),  # 10 terms, 5 relationships
            ("medium", 100, 25),  # 100 terms, 25 relationships
            ("large", 500, 100),  # 500 terms, 100 relationships
            ("tiny", 2, 1),  # 2 terms, 1 relationship
            ("mediumtwo", 150, 30),  # 150 terms, 30 relationships
        ]

        file_paths = []
        expected_ontologies = []

        for name, term_count, rel_count in test_scenarios:
            # Create ontology with specified complexity
            terms = {}
            for i in range(term_count):
                term_id = f"{name.upper()}:{i+1:06d}"
                terms[term_id] = Term(
                    id=term_id,
                    name=f"{name} Term {i}",
                    definition=f"Definition for {name} term {i}",
                    namespace=f"{name}_namespace",
                )

            relationships = {}
            for i in range(rel_count):
                rel_id = f"{name.upper()}REL:{i+1:06d}"
                relationships[rel_id] = Relationship(
                    id=rel_id,
                    subject=f"{name.upper()}:{i+1:06d}",
                    predicate="participates_in",
                    object=f"{name.upper()}:{((i+1)%term_count)+1:06d}",
                    confidence=0.9,
                )

            ontology = Ontology(
                id=f"{name.upper()}PRIORITY:000001",
                name=f"{name.title()} Priority Test",
                version="1.0",
                terms=terms,
                relationships=relationships,
                namespaces=[f"{name}_namespace"],
            )
            expected_ontologies.append(ontology)

            # Create CSV file
            csv_lines = ["id,name,definition,namespace"]
            for term_id, term in terms.items():
                csv_lines.append(
                    f"{term_id},{term.name},{term.definition},{term.namespace}"
                )

            csv_file = temp_dir / f"priority_{name}.csv"
            csv_file.write_text("\n".join(csv_lines))
            file_paths.append(str(csv_file))

        with patch(
            "aim2_project.aim2_ontology.ontology_manager.auto_detect_parser"
        ) as mock_auto_detect:
            # Create parsers with different processing times based on complexity
            parsers = []
            for i, (name, term_count, rel_count) in enumerate(test_scenarios):
                parser = Mock()
                parser.format_name = "csv"
                # Simulate longer processing time for larger ontologies
                processing_time = 0.1 + (term_count / 1000.0)
                parser.parse.return_value = ParseResult(
                    success=True,
                    data=expected_ontologies[i],
                    errors=[],
                    warnings=[],
                    metadata={
                        "complexity": term_count + rel_count,
                        "processing_time": processing_time,
                    },
                    parse_time=processing_time,
                )
                parsers.append(parser)

            mock_auto_detect.side_effect = parsers

            # Test different loading strategies

            # Strategy 1: Load all at once (baseline)
            start_time = time.time()
            results_all = ontology_manager.load_ontologies(file_paths)
            total_time_all = time.time() - start_time

            assert all(r.success for r in results_all)
            assert len(results_all) == len(file_paths)

            # Strategy 2: Load in priority order (smallest first for faster feedback)
            ontology_manager.ontologies.clear()
            ontology_manager.clear_cache()

            # Sort by complexity (smallest first)
            complexity_sorted = sorted(
                enumerate(test_scenarios), key=lambda x: x[1][1] + x[1][2]
            )
            priority_file_paths = [file_paths[i] for i, _ in complexity_sorted]

            # Reset mock for second test
            mock_auto_detect.side_effect = [parsers[i] for i, _ in complexity_sorted]

            start_time = time.time()
            results_priority = ontology_manager.load_ontologies(priority_file_paths)
            total_time_priority = time.time() - start_time

            assert all(r.success for r in results_priority)

            # Verify that smaller ontologies loaded first would give faster initial feedback
            # (This is more of a conceptual test - in practice, you might implement actual prioritization)

            # Strategy 3: Batch loading with size limits (simulate chunked processing)
            ontology_manager.ontologies.clear()
            ontology_manager.clear_cache()

            chunk_size = 2
            chunks = [
                file_paths[i : i + chunk_size]
                for i in range(0, len(file_paths), chunk_size)
            ]

            # Reset mock for chunked test
            mock_auto_detect.side_effect = (
                parsers * 2
            )  # Ensure enough parsers for all chunks

            chunked_results = []
            start_time = time.time()
            for chunk in chunks:
                chunk_results = ontology_manager.load_ontologies(chunk)
                chunked_results.extend(chunk_results)
                # Small delay between chunks (simulate processing time)
                time.sleep(0.01)
            total_time_chunked = time.time() - start_time

            assert len(chunked_results) == len(file_paths)
            assert all(r.success for r in chunked_results)

            # Verify statistics for different strategies
            stats = ontology_manager.get_statistics()
            assert stats["total_loads"] >= len(
                file_paths
            )  # May be higher due to multiple strategies

            # All strategies should complete in reasonable time
            max_reasonable_time = 5.0
            assert total_time_all < max_reasonable_time
            assert total_time_priority < max_reasonable_time
            assert total_time_chunked < max_reasonable_time

    def test_error_recovery_and_resilience(self, ontology_manager, temp_dir):
        """Test error recovery and resilience during multi-source loading."""
        # Create a mix of files with different types of issues
        sources_and_behaviors = []

        # Good file that should always work
        good_file = temp_dir / "good.csv"
        good_file.write_text("id,name,definition\nGOOD:001,Good Term,Good Definition")
        sources_and_behaviors.append((str(good_file), "success"))

        # File that fails initially but succeeds on retry
        retry_file = temp_dir / "retry.csv"
        retry_file.write_text(
            "id,name,definition\nRETRY:001,Retry Term,Retry Definition"
        )
        sources_and_behaviors.append((str(retry_file), "retry_success"))

        # File that always fails
        bad_file = temp_dir / "bad.csv"
        bad_file.write_text("invalid,csv,content,without,proper,structure")
        sources_and_behaviors.append((str(bad_file), "always_fail"))

        # File with intermittent issues
        intermittent_file = temp_dir / "intermittent.csv"
        intermittent_file.write_text(
            "id,name,definition\nINTER:001,Intermittent Term,Intermittent Definition"
        )
        sources_and_behaviors.append((str(intermittent_file), "intermittent"))

        # File that succeeds with warnings
        warning_file = temp_dir / "warning.csv"
        warning_file.write_text(
            "id,name,definition\nWARN:001,Warning Term,Warning Definition"
        )
        sources_and_behaviors.append((str(warning_file), "success_with_warnings"))

        sources = [source for source, _ in sources_and_behaviors]

        with patch(
            "aim2_project.aim2_ontology.ontology_manager.auto_detect_parser"
        ) as mock_auto_detect:
            call_counts = {}

            def mock_parser_with_behavior(file_path=None, **kwargs):
                file_path_str = str(file_path) if file_path else ""

                # Track call counts for retry logic
                if file_path_str not in call_counts:
                    call_counts[file_path_str] = 0
                call_counts[file_path_str] += 1

                parser = Mock()
                parser.format_name = "resilient_csv"

                if "good.csv" in file_path_str:
                    # Always succeeds
                    parser.parse.return_value = ParseResult(
                        success=True,
                        data=Ontology(
                            id="GOOD:001",
                            name="Good",
                            version="1.0",
                            terms={},
                            relationships={},
                            namespaces=[],
                        ),
                        errors=[],
                        warnings=[],
                        metadata={},
                        parse_time=0.1,
                    )
                    return parser

                elif "retry.csv" in file_path_str:
                    # Fails first time, succeeds second time
                    if call_counts[file_path_str] == 1:
                        parser.parse.return_value = ParseResult(
                            success=False,
                            data=None,
                            errors=["Temporary parsing error"],
                            warnings=[],
                            metadata={},
                            parse_time=0.05,
                        )
                    else:
                        parser.parse.return_value = ParseResult(
                            success=True,
                            data=Ontology(
                                id="RETRY:001",
                                name="Retry",
                                version="1.0",
                                terms={},
                                relationships={},
                                namespaces=[],
                            ),
                            errors=[],
                            warnings=[],
                            metadata={},
                            parse_time=0.1,
                        )
                    return parser

                elif "bad.csv" in file_path_str:
                    # Always fails
                    parser.parse.return_value = ParseResult(
                        success=False,
                        data=None,
                        errors=["Persistent parsing error", "Invalid CSV structure"],
                        warnings=[],
                        metadata={},
                        parse_time=0.02,
                    )
                    return parser

                elif "intermittent.csv" in file_path_str:
                    # Succeeds every other time
                    if call_counts[file_path_str] % 2 == 1:
                        parser.parse.return_value = ParseResult(
                            success=False,
                            data=None,
                            errors=["Intermittent network error"],
                            warnings=[],
                            metadata={},
                            parse_time=0.03,
                        )
                    else:
                        parser.parse.return_value = ParseResult(
                            success=True,
                            data=Ontology(
                                id="INTER:001",
                                name="Intermittent",
                                version="1.0",
                                terms={},
                                relationships={},
                                namespaces=[],
                            ),
                            errors=[],
                            warnings=[],
                            metadata={},
                            parse_time=0.1,
                        )
                    return parser

                elif "warning.csv" in file_path_str:
                    # Succeeds but with warnings
                    parser.parse.return_value = ParseResult(
                        success=True,
                        data=Ontology(
                            id="WARN:001",
                            name="Warning",
                            version="1.0",
                            terms={},
                            relationships={},
                            namespaces=[],
                        ),
                        errors=[],
                        warnings=["Deprecated field format", "Missing optional field"],
                        metadata={},
                        parse_time=0.1,
                    )
                    return parser

                return None

            mock_auto_detect.side_effect = mock_parser_with_behavior

            # First attempt - some may fail
            results1 = ontology_manager.load_ontologies(sources)

            # Count outcomes
            successes1 = [r for r in results1 if r.success]
            failures1 = [r for r in results1 if not r.success]

            # Should have at least some successes and some failures
            assert (
                len(successes1) >= 2
            ), "Should have at least 2 successes on first attempt"
            assert (
                len(failures1) >= 1
            ), "Should have at least 1 failure on first attempt"

            # Retry failed sources (simulate retry logic)
            failed_sources = [r.source_path for r in failures1 if r.source_path]

            if failed_sources:
                # Reset mock call behavior for retry
                results_retry = ontology_manager.load_ontologies(failed_sources)

                # Some retries should succeed (retry.csv, intermittent.csv on even calls)
                retry_successes = [r for r in results_retry if r.success]
                assert len(retry_successes) >= 1, "At least some retries should succeed"

            # Verify resilience - manager should still be operational
            stats = ontology_manager.get_statistics()
            assert stats["total_loads"] > 0
            assert stats["successful_loads"] > 0
            assert (
                len(ontology_manager.ontologies) >= 2
            )  # At least good and warning files

            # Test warning handling
            warning_results = [r for r in results1 if r.success and r.warnings]
            assert (
                len(warning_results) >= 1
            ), "Should have at least one result with warnings"

            # Verify warning result details
            for result in warning_results:
                assert len(result.warnings) > 0
                assert (
                    result.ontology is not None
                )  # Should still have loaded despite warnings

    def test_progressive_loading_feedback(self, ontology_manager, temp_dir):
        """Test progressive loading with feedback mechanisms."""
        # Create multiple files for progressive loading
        file_count = 8
        file_paths = []
        expected_ontologies = []

        for i in range(file_count):
            ontology = Ontology(
                id=f"PROGRESSIVE:{i:06d}",
                name=f"Progressive Ontology {i}",
                version="1.0",
                terms={
                    f"PROG:{i*100+j:06d}": Term(
                        id=f"PROG:{i*100+j:06d}",
                        name=f"Progressive Term {j}",
                        definition=f"Definition {j}",
                        namespace=f"progressive_{i}",
                    )
                    for j in range(20)
                },  # 20 terms each
                relationships={},
                namespaces=[f"progressive_{i}"],
            )
            expected_ontologies.append(ontology)

            csv_file = temp_dir / f"progressive_{i}.csv"
            csv_file.write_text(
                f"id,name,definition\nPROG:{i*100+1:06d},Progressive Term,Progressive Definition"
            )
            file_paths.append(str(csv_file))

        # Track loading progress
        progress_callbacks = []

        def progress_callback(completed, total, current_file=None, result=None):
            progress_callbacks.append(
                {
                    "completed": completed,
                    "total": total,
                    "current_file": current_file,
                    "result": result,
                    "timestamp": time.time(),
                }
            )

        with patch(
            "aim2_project.aim2_ontology.ontology_manager.auto_detect_parser"
        ) as mock_auto_detect:
            parsers = [
                Mock(
                    format_name="csv",
                    parse=Mock(
                        return_value=ParseResult(
                            success=True,
                            data=ont,
                            errors=[],
                            warnings=[],
                            metadata={},
                            parse_time=0.1,
                        )
                    ),
                )
                for ont in expected_ontologies
            ]
            mock_auto_detect.side_effect = parsers

            # Simulate progressive loading with callbacks
            # Note: This would require modification to OntologyManager to support callbacks
            # For this test, we'll simulate the concept

            results = []
            start_time = time.time()

            for i, file_path in enumerate(file_paths):
                # Simulate progress callback
                progress_callback(i, len(file_paths), file_path, None)

                # Load individual file
                result = ontology_manager.load_ontology(file_path)
                results.append(result)

                # Callback with result
                progress_callback(i + 1, len(file_paths), file_path, result)

                # Small delay to simulate processing time
                time.sleep(0.01)

            end_time = time.time()

            # Verify progressive loading worked
            assert len(results) == file_count
            assert all(r.success for r in results)

            # Verify progress tracking
            assert (
                len(progress_callbacks) == file_count * 2
            )  # Before and after each file

            # Check progress increments
            completion_callbacks = [
                cb for cb in progress_callbacks if cb["result"] is not None
            ]
            assert len(completion_callbacks) == file_count

            for i, callback in enumerate(completion_callbacks):
                assert callback["completed"] == i + 1
                assert callback["total"] == file_count
                assert callback["result"].success is True

            # Verify timing progression
            timestamps = [cb["timestamp"] for cb in progress_callbacks]
            assert timestamps == sorted(
                timestamps
            ), "Timestamps should be in chronological order"

            # Total time should be reasonable
            total_time = end_time - start_time
            assert (
                total_time < 2.0
            ), f"Progressive loading took too long: {total_time:.2f}s"

            # Verify all ontologies were loaded
            assert len(ontology_manager.ontologies) == file_count

            # Verify statistics reflect progressive loading
            stats = ontology_manager.get_statistics()
            assert stats["total_loads"] == file_count
            assert stats["successful_loads"] == file_count
            assert stats["failed_loads"] == 0

    def test_load_from_config_multi_source(self, ontology_manager, temp_dir):
        """Test loading multiple ontologies from configuration file."""
        # Create mock configuration data
        mock_config_data = {
            "ontology": {
                "sources": {
                    "chebi": {
                        "enabled": True,
                        "local_path": str(temp_dir / "chebi.owl"),
                        "url": "https://example.com/chebi.owl",
                        "format": "owl",
                        "description": "Chemical Entities of Biological Interest"
                    },
                    "chemont": {
                        "enabled": True,
                        "local_path": str(temp_dir / "chemont.obo"),
                        "url": "https://example.com/chemont.obo",
                        "format": "obo",
                        "description": "Chemical Ontology"
                    },
                    "peco": {
                        "enabled": True,
                        "local_path": str(temp_dir / "peco.owl"),
                        "url": "https://example.com/peco.owl",
                        "format": "owl",
                        "description": "Plant Experimental Conditions Ontology"
                    },
                    "trait_ontology": {
                        "enabled": True,
                        "local_path": str(temp_dir / "to.owl"),
                        "url": "https://example.com/to.owl",
                        "format": "owl",
                        "description": "Plant Trait Ontology"
                    },
                    "pmn": {
                        "enabled": False,  # Disabled for this test
                        "local_path": str(temp_dir / "pmn.owl"),
                        "url": "https://example.com/pmn.owl",
                        "format": "biopax_owl",
                        "description": "Plant Metabolic Network"
                    },
                    "chemfont": {
                        "enabled": True,
                        "local_path": str(temp_dir / "chemfont.owl"),
                        "url": "https://example.com/chemfont.owl",
                        "format": "owl",
                        "description": "Chemical Functional Ontology"
                    }
                }
            }
        }

        # Create test ontology files
        test_ontologies = {}
        for source_name, config in mock_config_data["ontology"]["sources"].items():
            if config["enabled"]:
                file_path = Path(config["local_path"])
                
                # Create sample ontology
                ontology = Ontology(
                    id=f"{source_name.replace('_', '').upper()}:000001",
                    name=f"{source_name.title()} Test Ontology",
                    version="1.0",
                    description=config["description"],
                    terms={
                        f"{source_name.replace('_', '').upper()}:{i:06d}": Term(
                            id=f"{source_name.replace('_', '').upper()}:{i:06d}",
                            name=f"{source_name} Term {i}",
                            definition=f"Test term {i} from {source_name}",
                            namespace=f"{source_name}_namespace",
                        )
                        for i in range(1, 6)  # 5 terms each
                    },
                    relationships={},
                    namespaces=[f"{source_name}_namespace"],
                )
                test_ontologies[source_name] = ontology
                
                # Create dummy file content
                if config["format"] == "owl":
                    content = f"""<?xml version="1.0"?>
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:owl="http://www.w3.org/2002/07/owl#"
         xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#">
    <owl:Ontology rdf:about="http://example.org/{source_name}">
        <rdfs:label>{ontology.name}</rdfs:label>
        <rdfs:comment>{ontology.description}</rdfs:comment>
    </owl:Ontology>
</rdf:RDF>"""
                elif config["format"] == "obo":
                    content = f"""format-version: 1.2
ontology: {source_name}
default-namespace: {source_name}_namespace

[Term]
id: {source_name.upper()}:000001
name: {source_name} Term 1
def: "Test term 1 from {source_name}" []
"""
                else:
                    content = f"# {source_name} ontology file content"
                
                file_path.write_text(content)

        # Mock ConfigManager
        mock_config_manager = Mock()
        mock_config_manager.get.return_value = mock_config_data["ontology"]["sources"]

        with patch("aim2_project.aim2_ontology.ontology_manager.auto_detect_parser") as mock_auto_detect:
            # Create parsers for enabled sources
            parsers = []
            for source_name, config in mock_config_data["ontology"]["sources"].items():
                if config["enabled"]:
                    parser = Mock()
                    parser.format_name = config["format"]
                    parser.parse.return_value = ParseResult(
                        success=True,
                        data=test_ontologies[source_name],
                        errors=[],
                        warnings=[],
                        metadata={"format": config["format"]},
                        parse_time=0.1,
                    )
                    parsers.append(parser)
            
            mock_auto_detect.side_effect = parsers

            # Load from configuration
            results = ontology_manager.load_from_config(
                config_manager=mock_config_manager,
                enabled_only=True
            )

            # Verify results
            enabled_sources = [name for name, config in mock_config_data["ontology"]["sources"].items() if config["enabled"]]
            assert len(results) == len(enabled_sources)
            assert all(r.success for r in results)

            # Verify all enabled ontologies were loaded
            assert len(ontology_manager.ontologies) == len(enabled_sources)

            # Verify specific sources were loaded
            for source_name in enabled_sources:
                found = False
                for ontology in ontology_manager.ontologies.values():
                    if source_name.replace('_', '').upper() in ontology.id:
                        found = True
                        break
                assert found, f"Ontology from {source_name} not found in loaded ontologies"

            # Verify statistics
            stats = ontology_manager.get_statistics()
            assert stats["total_loads"] == len(enabled_sources)
            assert stats["successful_loads"] == len(enabled_sources)
            assert stats["failed_loads"] == 0

    def test_source_metadata_tracking_multi_source(self, ontology_manager, temp_dir):
        """Test source metadata tracking across multiple sources."""
        # Create multiple test files with different characteristics
        sources_metadata = [
            {
                "name": "sourceone",
                "format": "owl", 
                "size": "large",
                "terms_count": 100,
                "file_ext": ".owl"
            },
            {
                "name": "sourcetwo", 
                "format": "csv",
                "size": "medium", 
                "terms_count": 50,
                "file_ext": ".csv"
            },
            {
                "name": "sourcethree",
                "format": "jsonld",
                "size": "small",
                "terms_count": 25, 
                "file_ext": ".jsonld"
            }
        ]

        file_paths = []
        expected_ontologies = []

        for meta in sources_metadata:
            # Create ontology with specified characteristics
            terms = {}
            for i in range(meta["terms_count"]):
                term_id = f"{meta['name'].replace('_', '').upper()}:{i+1:06d}"
                terms[term_id] = Term(
                    id=term_id,
                    name=f"{meta['name']} Term {i+1}",
                    definition=f"Test term {i+1} from {meta['name']}",
                    namespace=f"{meta['name']}_namespace",
                )

            ontology = Ontology(
                id=f"{meta['name'].replace('_', '').upper()}TRACK:000001",
                name=f"{meta['name']} Tracking Test",
                version="1.0",
                description=f"Test ontology for metadata tracking - {meta['size']} size",
                terms=terms,
                relationships={},
                namespaces=[f"{meta['name']}_namespace"],
            )
            expected_ontologies.append(ontology)

            # Create test file
            file_path = temp_dir / f"{meta['name']}_metadata{meta['file_ext']}"
            file_path.write_text(f"# {meta['name']} test content")
            file_paths.append(str(file_path))

        with patch("aim2_project.aim2_ontology.ontology_manager.auto_detect_parser") as mock_auto_detect:
            # Create parsers with metadata
            parsers = []
            for i, meta in enumerate(sources_metadata):
                parser = Mock()
                parser.format_name = meta["format"]
                parser.parse.return_value = ParseResult(
                    success=True,
                    data=expected_ontologies[i],
                    errors=[],
                    warnings=[],
                    metadata={
                        "format": meta["format"],
                        "size_category": meta["size"],
                        "terms_parsed": meta["terms_count"],
                        "source_name": meta["name"],
                        "processing_notes": f"Processed {meta['terms_count']} terms"
                    },
                    parse_time=0.1 + (meta["terms_count"] / 1000.0),  # Variable parse time based on size
                )
                parsers.append(parser)

            mock_auto_detect.side_effect = parsers

            # Load all sources
            results = ontology_manager.load_ontologies(file_paths)

            # Verify all succeeded
            assert all(r.success for r in results)
            assert len(results) == len(sources_metadata)

            # Verify source metadata tracking
            for i, result in enumerate(results):
                meta = sources_metadata[i]
                
                # Check result metadata (OntologyManager provides these standard fields)
                assert result.metadata["format"] == meta["format"]
                assert result.metadata["terms_count"] == meta["terms_count"]
                assert "parse_time" in result.metadata
                
                # Verify source path tracking
                assert result.source_path == file_paths[i]
                
                # Verify load time tracking (should be reasonable)
                assert result.load_time > 0, "Load time should be positive"
                assert result.load_time < 1.0, "Load time should be reasonable for mocked operations"

            # Verify source-specific statistics if available
            if hasattr(ontology_manager, 'source_stats'):
                assert len(ontology_manager.source_stats) == len(file_paths)
                for file_path in file_paths:
                    assert file_path in ontology_manager.source_stats

    def test_configuration_validation_multi_source(self, ontology_manager):
        """Test configuration validation for multiple sources."""
        # Test various configuration validation scenarios
        
        # Test 1: Valid configuration
        valid_config = {
            "ontology": {
                "sources": {
                    "valid_source": {
                        "enabled": True,
                        "local_path": "/tmp/valid.owl",
                        "url": "https://example.com/valid.owl",
                        "format": "owl"
                    }
                }
            }
        }
        
        mock_config_manager = Mock()
        mock_config_manager.get.return_value = valid_config["ontology"]["sources"]
        
        # Should not raise exception for valid config
        filtered_sources = ontology_manager._filter_configured_sources(
            valid_config["ontology"]["sources"],
            source_filter=None,
            enabled_only=True
        )
        assert len(filtered_sources) == 1
        assert "valid_source" in filtered_sources

        # Test 2: Empty configuration
        empty_config = {}
        mock_config_manager.get.return_value = empty_config
        
        filtered_sources = ontology_manager._filter_configured_sources(
            empty_config,
            source_filter=None,
            enabled_only=True
        )
        assert len(filtered_sources) == 0

        # Test 3: All disabled sources
        disabled_config = {
            "disabled_source1": {
                "enabled": False,
                "local_path": "/tmp/disabled1.owl",
                "format": "owl"
            },
            "disabled_source2": {
                "enabled": False,
                "local_path": "/tmp/disabled2.owl", 
                "format": "owl"
            }
        }
        
        filtered_sources = ontology_manager._filter_configured_sources(
            disabled_config,
            source_filter=None,
            enabled_only=True
        )
        assert len(filtered_sources) == 0
        
        # But should include disabled when enabled_only=False
        filtered_sources = ontology_manager._filter_configured_sources(
            disabled_config,
            source_filter=None,
            enabled_only=False
        )
        assert len(filtered_sources) == 2

        # Test 4: Source filtering
        mixed_config = {
            "source_a": {"enabled": True, "local_path": "/tmp/a.owl", "format": "owl"},
            "source_b": {"enabled": True, "local_path": "/tmp/b.owl", "format": "owl"},
            "source_c": {"enabled": True, "local_path": "/tmp/c.owl", "format": "owl"}
        }
        
        # Filter for specific sources
        filtered_sources = ontology_manager._filter_configured_sources(
            mixed_config,
            source_filter=["source_a", "source_c"],
            enabled_only=True
        )
        assert len(filtered_sources) == 2
        assert "source_a" in filtered_sources
        assert "source_c" in filtered_sources
        assert "source_b" not in filtered_sources

        # Test 5: Invalid source filter (non-existent source)
        filtered_sources = ontology_manager._filter_configured_sources(
            mixed_config,
            source_filter=["nonexistent_source"],
            enabled_only=True
        )
        assert len(filtered_sources) == 0

    def test_error_handling_mixed_success_failure_detailed(self, ontology_manager, temp_dir):
        """Test detailed error handling when some sources succeed and others fail."""
        # Create a mix of scenarios
        test_scenarios = [
            {"name": "success_owl", "should_succeed": True, "error_type": None},
            {"name": "missing_file", "should_succeed": False, "error_type": "file_not_found"},
            {"name": "success_csv", "should_succeed": True, "error_type": None},
            {"name": "parse_error", "should_succeed": False, "error_type": "parse_failure"},
            {"name": "no_parser", "should_succeed": False, "error_type": "no_parser"},
            {"name": "success_json", "should_succeed": True, "error_type": None},
        ]

        file_paths = []
        expected_ontologies = []

        for scenario in test_scenarios:
            if scenario["should_succeed"]:
                # Create valid file and expected ontology
                ontology = Ontology(
                    id=f"{scenario['name'].replace('_', '').upper()}:000001",
                    name=f"{scenario['name']} Success Test",
                    version="1.0",
                    terms={
                        f"{scenario['name'].replace('_', '').upper()}:000001": Term(
                            id=f"{scenario['name'].replace('_', '').upper()}:000001",
                            name=f"{scenario['name']} term",
                            definition=f"Test term for {scenario['name']}",
                            namespace=f"{scenario['name']}_ns",
                        )
                    },
                    relationships={},
                    namespaces=[f"{scenario['name']}_ns"],
                )
                expected_ontologies.append(ontology)
                
                file_path = temp_dir / f"{scenario['name']}.test"
                file_path.write_text(f"# {scenario['name']} content")
                file_paths.append(str(file_path))
            
            elif scenario["error_type"] == "file_not_found":
                # Non-existent file
                file_paths.append(str(temp_dir / f"{scenario['name']}_nonexistent.owl"))
            
            elif scenario["error_type"] == "parse_error":
                # File exists but parsing fails
                file_path = temp_dir / f"{scenario['name']}.corrupt"
                file_path.write_text("corrupted content that will fail parsing")
                file_paths.append(str(file_path))
            
            elif scenario["error_type"] == "no_parser":
                # File exists but no parser available
                file_path = temp_dir / f"{scenario['name']}.unknown_format"
                file_path.write_text("content in unknown format")
                file_paths.append(str(file_path))

        with patch("aim2_project.aim2_ontology.ontology_manager.auto_detect_parser") as mock_auto_detect:
            def mock_parser_behavior(file_path=None, **kwargs):
                file_path_str = str(file_path) if file_path else ""
                
                # Find matching scenario
                scenario = None
                for s in test_scenarios:
                    if s["name"] in file_path_str:
                        scenario = s
                        break
                
                # Debug output
                print(f"Mock called for: {file_path_str}, matched scenario: {scenario['name'] if scenario else None}")
                
                if not scenario:
                    return None
                
                if scenario["should_succeed"]:
                    # Create successful parser
                    parser = Mock()
                    parser.format_name = "test_format"
                    
                    # Get expected ontology
                    ontology = None
                    successful_index = 0
                    for i, s in enumerate(test_scenarios):
                        if s["name"] == scenario["name"]:
                            # Find the index in expected_ontologies (only successful scenarios)
                            if s["should_succeed"]:
                                ontology = expected_ontologies[successful_index]
                            break
                        elif s["should_succeed"]:
                            successful_index += 1
                    
                    parser.parse.return_value = ParseResult(
                        success=True,
                        data=ontology,
                        errors=[],
                        warnings=[],
                        metadata={"scenario": scenario["name"]},
                        parse_time=0.1,
                    )
                    return parser
                
                elif scenario["error_type"] == "file_not_found":
                    # This case is handled by file existence check before parser creation
                    return None
                
                elif scenario["error_type"] == "parse_error":
                    # Parser exists but fails
                    parser = Mock()
                    parser.format_name = "corrupt_format"
                    parser.parse.return_value = ParseResult(
                        success=False,
                        data=None,
                        errors=[f"Parse error in {scenario['name']}: corrupted content"],
                        warnings=[],
                        metadata={},
                        parse_time=0.05,
                    )
                    return parser
                
                elif scenario["error_type"] == "no_parser":
                    # No parser available
                    return None
                
                return None

            mock_auto_detect.side_effect = mock_parser_behavior

            # Load all sources with mixed results
            results = ontology_manager.load_ontologies(file_paths)

            # Verify we got results for all files
            assert len(results) == len(file_paths)

            # Analyze results by scenario
            success_count = 0
            failure_count = 0
            error_types_found = set()

            for i, result in enumerate(results):
                scenario = test_scenarios[i]
                
                if scenario["should_succeed"]:
                    assert result.success, f"Expected success for {scenario['name']}, got errors: {result.errors}"
                    assert result.ontology is not None
                    assert result.source_path == file_paths[i]
                    success_count += 1
                else:
                    assert not result.success, f"Expected failure for {scenario['name']} but got success"
                    assert len(result.errors) > 0
                    assert result.ontology is None
                    failure_count += 1
                    
                    # Track error types
                    if scenario["error_type"] == "file_not_found":
                        error_types_found.add("file_not_found")
                        # Should contain parser-related error message (since file doesn't exist, no parser can be found)
                        assert any("no suitable parser" in error.lower() for error in result.errors)
                    elif scenario["error_type"] == "parse_failure":
                        error_types_found.add("parse_failure")
                        # Debug: print actual errors
                        print(f"Parse failure errors for {scenario['name']}: {result.errors}")
                        assert any("parse error" in error.lower() or "failed to parse" in error.lower() for error in result.errors)
                    elif scenario["error_type"] == "no_parser":
                        error_types_found.add("no_parser")
                        assert any("no suitable parser" in error.lower() for error in result.errors)

            # Verify counts
            expected_successes = sum(1 for s in test_scenarios if s["should_succeed"])
            expected_failures = len(test_scenarios) - expected_successes
            
            assert success_count == expected_successes
            assert failure_count == expected_failures

            # Verify only successful ontologies were stored
            assert len(ontology_manager.ontologies) == expected_successes

            # Verify comprehensive statistics
            stats = ontology_manager.get_statistics()
            assert stats["total_loads"] == len(file_paths)
            assert stats["successful_loads"] == expected_successes
            assert stats["failed_loads"] == expected_failures

            # Verify different error types were encountered
            assert len(error_types_found) >= 2, "Should have encountered multiple error types"

    def test_new_configuration_sources_validation(self, ontology_manager):
        """Test validation of new configuration sources (ChemOnt, PMN, PECO, Trait Ontology, ChemFont)."""
        # Test configuration for all new sources
        new_sources_config = {
            "chemont": {
                "enabled": True,
                "url": "http://classyfire.wishartlab.com/system/downloads/1_0/chemont/ChemOnt_2_1.obo.zip",
                "local_path": "./data/ontologies/chemont.obo",
                "format": "obo",
                "description": "Chemical Ontology (ChemOnt) - comprehensive chemical taxonomy",
                "update_frequency": "quarterly"
            },
            "pmn": {
                "enabled": True,
                "url": "https://ftp.plantcyc.org/",
                "local_path": "./data/ontologies/pmn_biopax.owl",
                "format": "biopax_owl",
                "description": "Plant Metabolic Network - plant metabolism pathways and compounds",
                "license_required": True,
                "contact": "curator@plantcyc.org",
                "update_frequency": "monthly"
            },
            "peco": {
                "enabled": True,
                "url": "http://purl.obolibrary.org/obo/peco.owl",
                "local_path": "./data/ontologies/peco.owl",
                "format": "owl",
                "description": "Plant Experimental Conditions Ontology - experimental treatments and conditions",
                "update_frequency": "monthly"
            },
            "trait_ontology": {
                "enabled": True,
                "url": "http://purl.obolibrary.org/obo/to.owl",
                "local_path": "./data/ontologies/to.owl",
                "format": "owl",
                "description": "Plant Trait Ontology - phenotypic traits in plants",
                "update_frequency": "monthly"
            },
            "chemfont": {
                "enabled": True,
                "url": "https://www.chemfont.ca/",
                "local_path": "./data/ontologies/chemfont.owl",
                "format": "owl",
                "description": "Chemical Functional Ontology - functions and actions of biological chemicals",
                "update_frequency": "quarterly",
                "download_note": "Access OWL file via Downloads tab on website"
            }
        }

        # Test filtering all new sources
        filtered_sources = ontology_manager._filter_configured_sources(
            new_sources_config,
            source_filter=None,
            enabled_only=True
        )
        
        # All sources should be included (all enabled)
        assert len(filtered_sources) == 5
        expected_sources = {"chemont", "pmn", "peco", "trait_ontology", "chemfont"}
        assert set(filtered_sources.keys()) == expected_sources

        # Test filtering specific new sources
        chemical_sources = ontology_manager._filter_configured_sources(
            new_sources_config,
            source_filter=["chemont", "chemfont"],
            enabled_only=True
        )
        assert len(chemical_sources) == 2
        assert "chemont" in chemical_sources
        assert "chemfont" in chemical_sources

        # Test plant-specific sources
        plant_sources = ontology_manager._filter_configured_sources(
            new_sources_config,
            source_filter=["pmn", "peco", "trait_ontology"],
            enabled_only=True
        )
        assert len(plant_sources) == 3
        assert "pmn" in plant_sources
        assert "peco" in plant_sources
        assert "trait_ontology" in plant_sources

        # Test format validation
        format_counts = {}
        for source_name, config in new_sources_config.items():
            format_name = config["format"]
            format_counts[format_name] = format_counts.get(format_name, 0) + 1

        # Verify expected format distribution
        assert format_counts["owl"] == 3  # peco, trait_ontology, chemfont
        assert format_counts["obo"] == 1  # chemont
        assert format_counts["biopax_owl"] == 1  # pmn

        # Test configuration completeness for each source
        required_fields = ["enabled", "local_path", "format", "description"]
        for source_name, config in new_sources_config.items():
            for field in required_fields:
                assert field in config, f"Missing required field '{field}' in {source_name} configuration"
            
            # Validate format values
            valid_formats = ["owl", "obo", "biopax_owl", "csv", "jsonld"]
            assert config["format"] in valid_formats, f"Invalid format '{config['format']}' for {source_name}"
            
            # Validate boolean fields
            assert isinstance(config["enabled"], bool), f"'enabled' should be boolean for {source_name}"

        # Test mixed enable/disable scenarios
        mixed_config = new_sources_config.copy()
        mixed_config["pmn"]["enabled"] = False
        mixed_config["chemfont"]["enabled"] = False

        enabled_only = ontology_manager._filter_configured_sources(
            mixed_config,
            source_filter=None,
            enabled_only=True
        )
        assert len(enabled_only) == 3  # chemont, peco, trait_ontology

        all_sources = ontology_manager._filter_configured_sources(
            mixed_config,
            source_filter=None,
            enabled_only=False
        )
        assert len(all_sources) == 5  # All sources regardless of enabled status

    def test_statistics_calculation_multi_source_comprehensive(self, ontology_manager, temp_dir):
        """Test comprehensive statistics calculation with multi-source data."""
        # Create diverse ontologies with different characteristics
        test_data = [
            {"name": "small", "terms": 10, "relationships": 5, "format": "owl"},
            {"name": "medium", "terms": 100, "relationships": 50, "format": "csv"},
            {"name": "large", "terms": 500, "relationships": 200, "format": "jsonld"},
            {"name": "empty", "terms": 0, "relationships": 0, "format": "owl"},
            {"name": "relationships_heavy", "terms": 50, "relationships": 150, "format": "obo"},
        ]

        file_paths = []
        expected_ontologies = []
        expected_total_terms = 0
        expected_total_relationships = 0

        for data in test_data:
            # Create ontology with specified size
            terms = {}
            for i in range(data["terms"]):
                term_id = f"{data['name'].replace('_', '').upper()}:{i+1:06d}"
                terms[term_id] = Term(
                    id=term_id,
                    name=f"{data['name']} Term {i+1}",
                    definition=f"Test term {i+1} from {data['name']} ontology",
                    namespace=f"{data['name']}_namespace",
                )
            expected_total_terms += data["terms"]

            relationships = {}
            for i in range(data["relationships"]):
                rel_id = f"{data['name'].replace('_', '').upper()}REL:{i+1:06d}"
                # Create relationships between terms (with wraparound for small ontologies)
                subject_idx = i % max(1, data["terms"])
                object_idx = (i + 1) % max(1, data["terms"])
                relationships[rel_id] = Relationship(
                    id=rel_id,
                    subject=f"{data['name'].replace('_', '').upper()}:{subject_idx+1:06d}",
                    predicate="relates_to",
                    object=f"{data['name'].replace('_', '').upper()}:{object_idx+1:06d}",
                    confidence=0.9,
                )
            expected_total_relationships += data["relationships"]

            ontology = Ontology(
                id=f"{data['name'].replace('_', '').upper()}STATS:000001",
                name=f"{data['name']} Statistics Test",
                version="1.0",
                description=f"Test ontology for statistics - {data['terms']} terms, {data['relationships']} relationships",
                terms=terms,
                relationships=relationships,
                namespaces=[f"{data['name']}_namespace"],
            )
            expected_ontologies.append(ontology)

            # Create test file
            file_path = temp_dir / f"{data['name']}_stats.{data['format']}"
            file_path.write_text(f"# {data['name']} statistics test content")
            file_paths.append(str(file_path))

        # Verify initial statistics state
        initial_stats = ontology_manager.get_statistics()
        assert initial_stats["total_loads"] == 0
        assert initial_stats["successful_loads"] == 0
        assert initial_stats["failed_loads"] == 0
        assert initial_stats["total_terms"] == 0
        assert initial_stats["total_relationships"] == 0
        assert initial_stats["loaded_ontologies"] == 0

        with patch("aim2_project.aim2_ontology.ontology_manager.auto_detect_parser") as mock_auto_detect:
            # Create parsers with varying processing times
            parsers = []
            for i, data in enumerate(test_data):
                parser = Mock()
                parser.format_name = data["format"]
                # Simulate longer processing time for larger ontologies
                processing_time = 0.05 + (data["terms"] + data["relationships"]) / 1000.0
                parser.parse.return_value = ParseResult(
                    success=True,
                    data=expected_ontologies[i],
                    errors=[],
                    warnings=[],
                    metadata={
                        "format": data["format"],
                        "complexity_score": data["terms"] + data["relationships"],
                        "processing_notes": f"Processed {data['terms']} terms and {data['relationships']} relationships"
                    },
                    parse_time=processing_time,
                )
                parsers.append(parser)

            mock_auto_detect.side_effect = parsers

            # Load all ontologies
            start_time = time.time()
            results = ontology_manager.load_ontologies(file_paths)
            total_load_time = time.time() - start_time

            # Verify all loaded successfully
            assert all(r.success for r in results)
            assert len(results) == len(test_data)

            # Get comprehensive statistics
            stats = ontology_manager.get_statistics()

            # Verify basic load statistics
            assert stats["total_loads"] == len(test_data)
            assert stats["successful_loads"] == len(test_data)
            assert stats["failed_loads"] == 0

            # Verify ontology content statistics
            assert stats["total_terms"] == expected_total_terms
            assert stats["total_relationships"] == expected_total_relationships
            assert stats["loaded_ontologies"] == len(test_data)

            # Verify format-specific statistics
            formats_loaded = stats["formats_loaded"]
            expected_format_counts = {}
            for data in test_data:
                fmt = data["format"]
                expected_format_counts[fmt] = expected_format_counts.get(fmt, 0) + 1

            for fmt, expected_count in expected_format_counts.items():
                assert formats_loaded[fmt] == expected_count, f"Format {fmt}: expected {expected_count}, got {formats_loaded.get(fmt, 0)}"

            # Verify cache statistics
            assert stats["cache_hits"] == 0  # First load, no cache hits
            assert stats["cache_misses"] == len(test_data)

            # Test statistics after reloading (should show cache hits)
            results2 = ontology_manager.load_ontologies(file_paths)
            stats2 = ontology_manager.get_statistics()

            # Total loads should have doubled
            assert stats2["total_loads"] == len(test_data) * 2
            # But successful loads should still show the ontology count
            assert stats2["successful_loads"] == len(test_data) * 2
            # Cache hits should now be present
            assert stats2["cache_hits"] == len(test_data)

            # Verify per-ontology statistics
            ontology_ids = list(ontology_manager.ontologies.keys())
            assert len(ontology_ids) == len(test_data)

            # Check that ontologies with different sizes are properly tracked
            small_found = False
            large_found = False
            empty_found = False
            
            for ontology in ontology_manager.ontologies.values():
                if "SMALL" in ontology.id:
                    assert len(ontology.terms) == 10
                    small_found = True
                elif "LARGE" in ontology.id:
                    assert len(ontology.terms) == 500
                    large_found = True
                elif "EMPTY" in ontology.id:
                    assert len(ontology.terms) == 0
                    empty_found = True

            assert small_found and large_found and empty_found, "Not all ontology sizes were found"

            # Test statistics with failures mixed in
            # Add some failing sources
            failing_sources = ["/nonexistent1.owl", "/nonexistent2.csv"]
            all_sources = file_paths + failing_sources

            # Clear cache and reset some stats for this test
            ontology_manager.clear_cache()
            ontology_manager.ontologies.clear()
            ontology_manager.load_stats = {
                "total_loads": 0,
                "successful_loads": 0,
                "failed_loads": 0,
                "cache_hits": 0,
                "cache_misses": 0,
                "formats_loaded": defaultdict(int),
            }

            # Reload with failures
            mock_auto_detect.side_effect = parsers + [None, None]  # None for failing sources
            results_with_failures = ontology_manager.load_ontologies(all_sources)

            stats_with_failures = ontology_manager.get_statistics()

            # Verify mixed statistics
            assert stats_with_failures["total_loads"] == len(all_sources)
            assert stats_with_failures["successful_loads"] == len(test_data)
            assert stats_with_failures["failed_loads"] == len(failing_sources)

            # Calculate success rate
            success_rate = stats_with_failures["successful_loads"] / stats_with_failures["total_loads"]
            expected_success_rate = len(test_data) / len(all_sources)
            assert abs(success_rate - expected_success_rate) < 0.01

            # Verify that only successful ontologies contribute to content statistics
            assert stats_with_failures["total_terms"] == expected_total_terms
            assert stats_with_failures["total_relationships"] == expected_total_relationships

    def test_cache_effectiveness_multi_source_comprehensive(self, ontology_manager, temp_dir):
        """Test comprehensive cache effectiveness with multi-source loading scenarios."""
        # Create test files for cache testing
        source_configs = [
            {"name": "cachetestone", "format": "owl", "size": "small"},
            {"name": "cachetesttwo", "format": "csv", "size": "medium"},
            {"name": "cachetestthree", "format": "jsonld", "size": "large"},
            {"name": "cachetestfour", "format": "obo", "size": "tiny"},
        ]

        file_paths = []
        expected_ontologies = []

        for config in source_configs:
            # Create sample ontology
            term_count = {"tiny": 5, "small": 20, "medium": 50, "large": 100}[config["size"]]
            
            terms = {}
            for i in range(term_count):
                term_id = f"{config['name'].replace('_', '').upper()}:{i+1:06d}"
                terms[term_id] = Term(
                    id=term_id,
                    name=f"{config['name']} Term {i+1}",
                    definition=f"Cache test term {i+1}",
                    namespace=f"{config['name']}_ns",
                )

            ontology = Ontology(
                id=f"{config['name'].replace('_', '').upper()}CACHE:000001",
                name=f"{config['name']} Cache Test",
                version="1.0",
                terms=terms,
                relationships={},
                namespaces=[f"{config['name']}_ns"],
            )
            expected_ontologies.append(ontology)

            # Create test file
            file_path = temp_dir / f"{config['name']}.{config['format']}"
            file_path.write_text(f"# {config['name']} cache test content")
            file_paths.append(str(file_path))

        with patch("aim2_project.aim2_ontology.ontology_manager.auto_detect_parser") as mock_auto_detect:
            # Create parsers that track call counts
            call_counts = []
            parsers = []

            for i, config in enumerate(source_configs):
                parser = Mock()
                parser.format_name = config["format"]
                
                # Track calls
                call_count = {"count": 0}
                call_counts.append(call_count)
                
                def create_parse_func(ontology, call_counter):
                    def parse_func(*args, **kwargs):
                        call_counter["count"] += 1
                        return ParseResult(
                            success=True,
                            data=ontology,
                            errors=[],
                            warnings=[],
                            metadata={
                                "cache_test": True,
                                "call_number": call_counter["count"]
                            },
                            parse_time=0.1,
                        )
                    return parse_func
                
                parser.parse = create_parse_func(expected_ontologies[i], call_count)
                parsers.append(parser)

            mock_auto_detect.side_effect = parsers

            # Test 1: Initial load (all cache misses)
            initial_stats = ontology_manager.get_statistics()
            assert initial_stats["cache_hits"] == 0
            assert initial_stats["cache_misses"] == 0

            results1 = ontology_manager.load_ontologies(file_paths)
            
            # Verify initial load
            assert all(r.success for r in results1)
            assert len(results1) == len(file_paths)
            
            stats1 = ontology_manager.get_statistics()
            assert stats1["cache_hits"] == 0
            assert stats1["cache_misses"] == len(file_paths)
            assert stats1["successful_loads"] == len(file_paths)

            # Verify parsers were called once each
            for call_count in call_counts:
                assert call_count["count"] == 1

            # Test 2: Immediate reload (all cache hits)
            results2 = ontology_manager.load_ontologies(file_paths)
            
            assert all(r.success for r in results2)
            assert all(r.metadata.get("cache_hit", False) for r in results2)
            
            stats2 = ontology_manager.get_statistics()
            assert stats2["cache_hits"] == len(file_paths)
            assert stats2["cache_misses"] == len(file_paths)  # Unchanged from first load
            assert stats2["total_loads"] == len(file_paths) * 2

            # Verify parsers were NOT called again (still count = 1)
            for call_count in call_counts:
                assert call_count["count"] == 1

            # Test 3: Partial overlap reload
            partial_paths = file_paths[:2]  # First 2 files
            new_path = str(temp_dir / "new_cache_test.owl")
            
            # Create new ontology and file
            new_ontology = Ontology(
                id="NEWCACHE:000001",
                name="New Cache Test",
                version="1.0",
                terms={"NEW:000001": Term(id="NEW:000001", name="New Term", definition="New definition", namespace="new_ns")},
                relationships={},
                namespaces=["new_ns"],
            )
            
            new_file = Path(new_path)
            new_file.write_text("# New cache test content")
            
            # Create parser for new file
            new_parser = Mock()
            new_parser.format_name = "owl"
            new_call_count = {"count": 0}
            
            def new_parse_func(*args, **kwargs):
                new_call_count["count"] += 1
                return ParseResult(
                    success=True,
                    data=new_ontology,
                    errors=[],
                    warnings=[],
                    metadata={"new_cache_test": True},
                    parse_time=0.1,
                )
            
            new_parser.parse = new_parse_func
            
            # Reset mock to handle new file
            mock_auto_detect.side_effect = [new_parser]
            
            mixed_paths = partial_paths + [new_path]
            initial_cache_hits = stats2["cache_hits"]
            initial_cache_misses = stats2["cache_misses"]
            
            results3 = ontology_manager.load_ontologies(mixed_paths)
            
            assert all(r.success for r in results3)
            
            stats3 = ontology_manager.get_statistics()
            # Should have 2 more cache hits (partial_paths) and 1 more miss (new_path)
            assert stats3["cache_hits"] == initial_cache_hits + 2
            assert stats3["cache_misses"] == initial_cache_misses + 1

            # Verify new parser was called once, old parsers still once
            assert new_call_count["count"] == 1
            for call_count in call_counts:
                assert call_count["count"] == 1

            # Test 4: Cache size limit behavior
            # Create manager with limited cache size
            limited_manager = OntologyManager(enable_caching=True, cache_size_limit=2)
            
            # Reset parsers for limited manager test
            for call_count in call_counts:
                call_count["count"] = 0
            
            mock_auto_detect.side_effect = parsers
            
            # Load all files (more than cache limit)
            results4 = limited_manager.load_ontologies(file_paths)
            
            assert all(r.success for r in results4)
            assert len(limited_manager.ontologies) == len(file_paths)
            
            # Cache should be limited to cache_size_limit
            assert len(limited_manager._cache) == limited_manager.cache_size_limit
            
            stats4 = limited_manager.get_statistics()
            assert stats4["cache_misses"] == len(file_paths)  # All initial loads are misses
            
            # Test that cache eviction works correctly
            # Reload first files - some should hit cache, others should miss due to eviction
            results5 = limited_manager.load_ontologies(file_paths[:2])
            stats5 = limited_manager.get_statistics()
            
            # With LRU eviction and size limit 2, behavior depends on implementation details
            # At minimum, we should see some cache activity
            assert stats5["total_loads"] > stats4["total_loads"]

            # Test 5: Cache invalidation and refresh
            original_manager = ontology_manager
            
            # Simulate file modification by clearing cache
            original_manager.clear_cache()
            
            # Reset parsers
            for call_count in call_counts:
                call_count["count"] = 0
                
            mock_auto_detect.side_effect = parsers
            
            # Reload should result in cache misses
            results6 = original_manager.load_ontologies(file_paths)
            
            assert all(r.success for r in results6)
            
            # Verify parsers were called again after cache clear
            for call_count in call_counts:
                assert call_count["count"] == 1
                
            # Final statistics verification
            final_stats = original_manager.get_statistics()
            assert final_stats["cache_misses"] > stats2["cache_misses"]  # More misses after cache clear

    def test_performance_stats_multi_source(self, ontology_manager, temp_dir):
        """Test performance statistics tracking for multi-source operations."""
        # Create ontologies with varying processing complexity
        complexity_scenarios = [
            {"name": "fast", "terms": 10, "processing_time": 0.05},
            {"name": "medium", "terms": 50, "processing_time": 0.15},
            {"name": "slow", "terms": 100, "processing_time": 0.30},
            {"name": "veryslow", "terms": 200, "processing_time": 0.50},
        ]

        file_paths = []
        expected_ontologies = []
        expected_times = []

        for scenario in complexity_scenarios:
            # Create ontology
            terms = {}
            for i in range(scenario["terms"]):
                term_id = f"{scenario['name'].replace('_', '').upper()}:{i+1:06d}"
                terms[term_id] = Term(
                    id=term_id,
                    name=f"{scenario['name']} Term {i+1}",
                    definition=f"Performance test term {i+1}",
                    namespace=f"{scenario['name']}_ns",
                )

            ontology = Ontology(
                id=f"{scenario['name'].replace('_', '').upper()}PERF:000001",
                name=f"{scenario['name']} Performance Test",
                version="1.0",
                terms=terms,
                relationships={},
                namespaces=[f"{scenario['name']}_ns"],
            )
            expected_ontologies.append(ontology)
            expected_times.append(scenario["processing_time"])

            # Create test file
            file_path = temp_dir / f"{scenario['name']}_perf.csv"
            file_path.write_text(f"# {scenario['name']} performance test content")
            file_paths.append(str(file_path))

        with patch("aim2_project.aim2_ontology.ontology_manager.auto_detect_parser") as mock_auto_detect:
            # Create parsers with specific processing times
            parsers = []
            for i, scenario in enumerate(complexity_scenarios):
                parser = Mock()
                parser.format_name = "csv"
                
                def create_slow_parse(ontology, delay):
                    def slow_parse(*args, **kwargs):
                        time.sleep(delay)  # Simulate processing time
                        return ParseResult(
                            success=True,
                            data=ontology,
                            errors=[],
                            warnings=[],
                            metadata={"processing_complexity": delay},
                            parse_time=delay,
                        )
                    return slow_parse
                
                parser.parse = create_slow_parse(expected_ontologies[i], scenario["processing_time"])
                parsers.append(parser)

            mock_auto_detect.side_effect = parsers

            # Load all ontologies and measure total time
            start_time = time.time()
            results = ontology_manager.load_ontologies(file_paths)
            total_time = time.time() - start_time

            # Verify all loaded successfully
            assert all(r.success for r in results)

            # Test individual load times
            for i, result in enumerate(results):
                expected_min_time = expected_times[i] - 0.05  # Allow some variance
                expected_max_time = expected_times[i] + 0.10
                assert expected_min_time <= result.load_time <= expected_max_time, \
                    f"Load time for {complexity_scenarios[i]['name']}: expected ~{expected_times[i]}, got {result.load_time}"

            # Test total time is reasonable (sum of individual times plus overhead)
            expected_total_min = sum(expected_times) - 0.1
            expected_total_max = sum(expected_times) + 0.5  # Allow for overhead
            assert expected_total_min <= total_time <= expected_total_max, \
                f"Total time: expected {expected_total_min}-{expected_total_max}, got {total_time}"

            # Test performance statistics if available
            if hasattr(ontology_manager, 'performance_stats'):
                perf_stats = ontology_manager.performance_stats
                
                # Average load time should be reasonable
                if perf_stats.get("average_load_time", 0) > 0:
                    expected_avg = sum(expected_times) / len(expected_times)
                    actual_avg = perf_stats["average_load_time"]
                    assert abs(actual_avg - expected_avg) < 0.1, \
                        f"Average load time: expected ~{expected_avg}, got {actual_avg}"
                
                # Fastest and slowest should be tracked
                if perf_stats.get("fastest_load_time") is not None:
                    assert perf_stats["fastest_load_time"] <= min(expected_times) + 0.1
                
                if perf_stats.get("slowest_load_time") is not None:
                    assert perf_stats["slowest_load_time"] >= max(expected_times) - 0.1

            # Test performance with caching (second load should be much faster)
            cached_start = time.time()
            cached_results = ontology_manager.load_ontologies(file_paths)
            cached_time = time.time() - cached_start

            # Cached load should be significantly faster
            assert cached_time < total_time / 2, \
                f"Cached load should be faster: original {total_time:.3f}s, cached {cached_time:.3f}s"

            # All results should indicate cache hits
            assert all(r.metadata.get("cache_hit", False) for r in cached_results)


if __name__ == "__main__":
    pytest.main([__file__])
